{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "153015c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb33da52",
   "metadata": {},
   "source": [
    "使用arange创建一个行向量x。这个行向量包含以0开始的前12个整数（默认创建整数）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed9b407c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.arange(12)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca41fc1",
   "metadata": {},
   "source": [
    "可以通过张量的shape属性来访问张量（沿每个轴的长度）的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49dbce5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db05eb4c",
   "metadata": {},
   "source": [
    "如果只想知道张量中元素的综述，即形状的所有元素乘积，可以检查它的大小（size）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73a83867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e81e0e",
   "metadata": {},
   "source": [
    "要想改变一个张量的形状而不改变元素数量和元素值，可以调用reshape函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa0ccc67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=x.reshape(3,4)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53614b6e",
   "metadata": {},
   "source": [
    "也可以通过-1来自动计算出维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b23a621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(-1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f98b99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(3,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e9d6c0",
   "metadata": {},
   "source": [
    "有时，我们希望使用全0、全1、其他常量，或者从特定分布中随机采样的数字来初始化矩阵。我们可以创建一个形状为（2,3,4）的张量，其中所有元素都设置为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f45b070d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((2,3,4)) # 注意两个括号"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb0c6d5",
   "metadata": {},
   "source": [
    "同样，我们可以创建一个形状为（2，3，4）的张量，其中所欲元素都设置为1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25d4cc14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((2,3,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81963459",
   "metadata": {},
   "source": [
    "生成随机数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15b09b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5149, -0.6680, -0.7535,  0.1487],\n",
       "        [ 0.6508,  0.4483,  2.3212,  1.4149],\n",
       "        [ 1.5409,  1.4272, -0.4532, -1.4534]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c5e75b",
   "metadata": {},
   "source": [
    "# 2.1.2 运算符"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adfb55c",
   "metadata": {},
   "source": [
    "加减乘除，按元素运算。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadc024d",
   "metadata": {},
   "source": [
    "把多个张量连结在一起，把它们端到端地叠起来形成一个更大的张量。\n",
    "下面的例子分别演示了当我们沿行（轴-0，形状的第一个元素）和按列(轴-1，形状的第二个元素)连结两个矩阵时的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83a29d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [ 2.,  1.,  4.,  3.],\n",
       "         [ 1.,  2.,  3.,  4.],\n",
       "         [ 4.,  3.,  2.,  1.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
       "         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=torch.arange(12,dtype=torch.float32).reshape((3,4))\n",
    "Y=torch.tensor([[2.0,1,4,3],[1,2,3,4],[4,3,2,1]])\n",
    "torch.cat((X,Y),dim=0),torch.cat((X,Y),dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afc22fc",
   "metadata": {},
   "source": [
    "# 2.1.3 广播机制\n",
    "在某些情况下，即使形状不同，我们仍然可以通过调用广播机制来执行按元素操作。\n",
    "这种机制的工作方式如下：首先，通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状。其次，对生成的数组执行按元素操作。\n",
    "在大多数情况下，我们将沿着数组中长度为1的轴进行广播，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7592132b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2]]),\n",
       " tensor([[0, 1]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.arange(3).reshape((3,1))\n",
    "b=torch.arange(2).reshape((1,2))\n",
    "a,b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b1b542",
   "metadata": {},
   "source": [
    "a,b两个矩阵直接相加，形状不匹配。将两个矩阵广播为一个更大的3*2矩阵。如下所示：矩阵a将复制列，矩阵b将复制行，然后再按元素相加。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3d3253e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 2],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f32b5e1",
   "metadata": {},
   "source": [
    "# 2.1.4 索引和切片\n",
    "第一个元素的索引是0；用[-1]选择最后一个元素，可以用[1:3]选择第二个和第三个元素："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1be1962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]),\n",
       " tensor([ 8.,  9., 10., 11.]),\n",
       " tensor([[ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,X[-1],X[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c3896c",
   "metadata": {},
   "source": [
    "如果想为多个元素赋相同的值，我们只需要索引所有元素，然后为它们赋值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa318163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12., 12., 12., 12.],\n",
       "        [12., 12., 12., 12.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:2,:]=12 # 第0，1行全部被赋值为12\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746b3624",
   "metadata": {},
   "source": [
    "# 2.1.5 节省内存\n",
    "运行一些操作可能会导致为新结果分配内存。 例如，如果我们用Y = X + Y，我们将取消引用Y指向的张量，而是指向新分配的内存处的张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef96c60",
   "metadata": {},
   "source": [
    "在下面的例子中，我们用Python的id()函数演示了这一点， 它给我们提供了内存中引用对象的确切地址。 运行Y = Y + X后，我们会发现id(Y)指向另一个位置。 这是因为Python首先计算Y + X，为结果分配新的内存，然后使Y指向内存中的这个新位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8211c3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before=id(Y)\n",
    "Y=Y+X\n",
    "id(Y)==before # 相加后的Y地址与相加前的地址不同"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53c19b6",
   "metadata": {},
   "source": [
    "这可能是不可取的，原因有两个：首先，我们不想总是不必要地分配内存。 在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。 通常情况下，我们希望原地执行这些更新。 其次，如果我们不原地更新，其他引用仍然会指向旧的内存位置， 这样我们的某些代码可能会无意中引用旧的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4dc838",
   "metadata": {},
   "source": [
    "幸运的是，执行原地操作非常简单。 我们可以使用切片表示法将操作的结果分配给先前分配的数组，例如Y[:] =\\<expression\\>。 为了说明这一点，我们首先创建一个新的矩阵Z，其形状与另一个Y相同， 使用zeros_like来分配一个全 0 的块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d956ac23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(Z): 1240793929184\n",
      "id(Z): 1240793929184\n"
     ]
    }
   ],
   "source": [
    "Z=torch.zeros_like(Y)\n",
    "print('id(Z):',id(Z))\n",
    "Z[:]=X+Y\n",
    "print('id(Z):',id(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809a3b39",
   "metadata": {},
   "source": [
    "如果在后续计算中没有重复使用X，我们也可以使用X[:]=X+Y或X+=Y来减少操作的内存开销。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b18ee2e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before=id(X)\n",
    "X+=Y\n",
    "id(X)==before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f49aacc",
   "metadata": {},
   "source": [
    "# 2.1.6 转换为其他python对象\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abfeb6c",
   "metadata": {},
   "source": [
    "转换为numpy张量（ndarray）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0390be5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, torch.Tensor)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=X.numpy()\n",
    "B=torch.tensor(A)\n",
    "type(A),type(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6175506",
   "metadata": {},
   "source": [
    "要将大小为1的张量转换为python标量，我们可以调用item函数或python的内置函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a50c6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.5000]), 3.5, 3.5, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.tensor([3.5])\n",
    "a,a.item(),float(a),int(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40049204",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
